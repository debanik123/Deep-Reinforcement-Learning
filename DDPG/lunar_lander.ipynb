{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('LunarLander-v2')\n",
    "'''from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K'''\n",
    "\n",
    "observation = env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "class Agent(object):\n",
    "    def __init__(self, alpha, gamma=0.99, n_actions=4, layer1_size=16,layer2_size=16, ip_dims =120, fname=\"ppo_rl.h5\"):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.lr= alpha\n",
    "        self.G = 0\n",
    "        self.ip_dims = ip_dims\n",
    "        self.fc1_dims = layer1_size\n",
    "        self.fc2_dims = layer2_size\n",
    "        self.n_actions = n_actions\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "        self.policy, self.predict = self.build_policy_network(alpha, n_actions, ip_dims, layer1_size, layer2_size)\n",
    "        \n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.model_file = fname\n",
    "        \n",
    "    \n",
    "    def build_policy_network(self, lr, n_actions, ip_dims, l1_size, l2_size):\n",
    "        ip = Input(shape=(self.ip_dims,))\n",
    "        advantages = Input(shape=[1])\n",
    "        dense1  = Dense(self.fc1_dims, activation='relu')(ip)\n",
    "        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n",
    "        probs = Dense(self.n_actions, activation='softmax')(dense2)\n",
    "        \n",
    "        def loss(y_true, y_pred):\n",
    "            out= K.clip(y_pred, 1e-8, 1-1e-8)\n",
    "            log_k = y_true*K.log(out)\n",
    "            \n",
    "            return K.sum(-log_k* advantages)\n",
    "        \n",
    "        policy =Model(inputs=[ip, advantages], outputs=[probs])\n",
    "        \n",
    "        policy.compile(optimizer=Adam(lr = self.lr), loss=loss)\n",
    "        \n",
    "        predict = Model(inputs=[ip], outputs=[probs])\n",
    "        \n",
    "        return policy, predict\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state =  observation[np.newaxis, :]\n",
    "        probabilities = self.predict.predict(state)[0]\n",
    "        action = np.random.choice(self.action_space, p=probabilities)    \n",
    "        return action\n",
    "    \n",
    "    def store_transition(self, observation, action, reward):\n",
    "        self.action_memory.append(action)\n",
    "        self.state_memory.append(observation)\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "    def learn(self):\n",
    "        state_memory = np.array(self.state_memory)\n",
    "        action_memory = np.array(self.action_memory)\n",
    "        reward_memory = np.array(self.reward_memory)\n",
    "        \n",
    "        action = np.zeros([len(action_memory), self.n_actions])\n",
    "        action[np.arange(len(action_memory)), action_memory] = 1\n",
    "        \n",
    "        G= np.zeros_like(reward_memory)\n",
    "        \n",
    "        for t in range(len(reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount  = 1\n",
    "            for k in range(t, len(reward_memory)):\n",
    "                G_sum += reward_memory[k]*discount\n",
    "                \n",
    "                discount *= self.gamma\n",
    "                \n",
    "            G[t] = G_sum\n",
    "            \n",
    "        mean  = np.mean(G)\n",
    "        std  = np.std(G) if np.std(G) >0 else 1\n",
    "        self.G = (G-mean)/std\n",
    "        \n",
    "        cost = self.policy.train_on_batch([state_memory, self.G], action)\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.policy.save(self.model_file)\n",
    "    def load_model(self):\n",
    "        self.policy = load_model(self.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent  = Agent(alpha=0.0005, gamma=0.99, n_actions=4, layer1_size=64, layer2_size=64, ip_dims=8)\n",
    "    \n",
    "    env = gym.make('LunarLander-v2')\n",
    "    score_history= []\n",
    "    \n",
    "    n_episodes = 2000\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        done = False\n",
    "        score = 0\n",
    "        obesrvation = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            action  = agent.choose_action(obesrvation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            agent.store_transition(obesrvation, action, reward)\n",
    "            \n",
    "            observation = observation_\n",
    "            \n",
    "            score += reward\n",
    "            \n",
    "        score_history.append(score)\n",
    "        \n",
    "        agent.learn()\n",
    "        print('episode', i, 'score %.1f'%score, 'average_score %.1f', np.mean(score_history[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent  = Agent(alpha=0.0005, gamma=0.99, n_actions=4, layer1_size=64, layer2_size=64, ip_dims=8)\n",
    "    \n",
    "    env = gym.make('LunarLander-v2')\n",
    "    score_history= []\n",
    "    \n",
    "    n_episodes = 2000\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        done = False\n",
    "        score = 0\n",
    "        obesrvation = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            env.render()\n",
    "            action  = agent.choose_action(obesrvation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            agent.store_transition(obesrvation, action, reward)\n",
    "            \n",
    "            observation = observation_\n",
    "            \n",
    "            score += reward\n",
    "            \n",
    "        score_history.append(score)\n",
    "        \n",
    "        agent.learn()\n",
    "        print('episode', i, 'score %.1f'%score, 'average_score %.1f', np.mean(score_history[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
