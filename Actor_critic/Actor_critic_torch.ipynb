{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_NN(nn.Module):\n",
    "    def __init__(self, alpha, ip_dims, n_actions):\n",
    "        super(G_NN, self).__init__()\n",
    "        self.ip_dims = ip_dims\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.ip_dims, 1024)\n",
    "        #self.b1  = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        #self.b2  = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        #self.b3  = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        #self.b4  = nn.BatchNorm1d(128)\n",
    "        self.fc5 = nn.Linear(128, 56)\n",
    "        #self.b5  = nn.BatchNorm1d(56)\n",
    "        self.fc6 = nn.Linear(56, self.n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        \n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, observation):\n",
    "        state = T.Tensor(observation).to(self.device)\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.b1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.b2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.b3(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.b4(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.b5(x)\n",
    "        \n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC_agent(nn.Module):\n",
    "    def __init__(self, alpha, ip_dims, n_actions):\n",
    "        super(AC_agent, self).__init__()\n",
    "        self.ip_dims = ip_dims\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.ip_dims, 1024)\n",
    "        #self.b1  = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        #self.b2  = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        #self.b3  = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        #self.b4  = nn.BatchNorm1d(128)\n",
    "        self.fc5 = nn.Linear(128, 56)\n",
    "        #self.b5  = nn.BatchNorm1d(56)\n",
    "        \n",
    "        self.pi = nn.Linear(56, self.n_actions)\n",
    "        self.v = nn.Linear(56, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        \n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
    "        self.to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, observation):\n",
    "        state = T.Tensor(observation).to(self.device)\n",
    "        x = F.relu(self.fc1(state))\n",
    "        #x = self.b1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = self.b2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #x = self.b3(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        #x = self.b4(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        #x = self.b5(x)\n",
    "        \n",
    "        pi = self.pi(x)\n",
    "        v = self.v(x)\n",
    "        return (pi, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object): \n",
    "    def __init__(self, alpha, beta, ip_dims, gamma=0.99, n_actions=2):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.actor = G_NN(alpha, ip_dims, n_actions)\n",
    "        self.critic = G_NN(alpha, ip_dims, 1)\n",
    "        \n",
    "        self.log_prob = None\n",
    "    \n",
    "    def choose_action(self, observations):\n",
    "        prob = F.softmax(self.actor.forward(observations))\n",
    "        action_prob = T.distributions.Categorical(prob)\n",
    "        action = action_prob.sample()\n",
    "        self.log_prob = action_prob.log_prob(action)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def learn(self, state, reward, new_state, done):\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        \n",
    "        critic_val_ = self.critic.forward(new_state)\n",
    "        critic_val = self.critic.forward(state)\n",
    "        \n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.actor.device)\n",
    "        \n",
    "        delta  = reward + self.gamma*critic_val_*(1-int(done)) - critic_val\n",
    "        \n",
    "        actor_loss = -self.log_prob*delta\n",
    "        critic_loss = delta**2\n",
    "        \n",
    "        (actor_loss + critic_loss).backward()\n",
    "        \n",
    "        self.actor.optimizer.step()\n",
    "        self.critic.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewAgent(object):\n",
    "    def __init__(self, alpha, ip_dims, gamma=0.99, n_actions=2):\n",
    "        self.gamma = gamma\n",
    "        self.AC = AC_agent(alpha, ip_dims, n_actions)\n",
    "        self.log_probs = None\n",
    "        \n",
    "    def choose_action(self, observations):\n",
    "        prob, _ = self.AC.forward(observations)\n",
    "        prob = F.softmax(prob)\n",
    "        \n",
    "        action_prob = T.distributions.Categorical(prob)\n",
    "        action  = action_prob.sample()\n",
    "        \n",
    "        log_probs = action_prob.log_prob(action)\n",
    "        self.log_probs =log_probs\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def learn(self, state, reward, new_state, done):\n",
    "        self.AC.optimizer.zero_grad()\n",
    "        \n",
    "        _, critic_val_ = self.AC.forward(new_state)\n",
    "        _, critic_val = self.AC.forward(state)\n",
    "        \n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.AC.device)\n",
    "        \n",
    "        delta  = reward + self.gamma*critic_val_*(1-int(done)) - critic_val\n",
    "        \n",
    "        actor_loss = -self.log_probs*delta\n",
    "        critic_loss = delta**2\n",
    "        \n",
    "        (actor_loss + critic_loss).backward()\n",
    "        \n",
    "        self.AC.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "def plotLearning(scores, filename, x=None, window=5):\n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(scores[max(0, t-window):(t+1)])\n",
    "    if x is None:\n",
    "        x = [i for i in range(N)]\n",
    "    plt.ylabel('Score')       \n",
    "    plt.xlabel('Game')                     \n",
    "    plt.plot(x, running_avg)\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    agent = NewAgent(alpha=0.000001, ip_dims=[8], gamma=0.9, n_actions=4)\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    score_his = []\n",
    "    num_ep = 2000\n",
    "    \n",
    "    for i in range(num_ep):\n",
    "        done = False\n",
    "        score = 0\n",
    "        obs = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            a = agent.choose_action(obs)\n",
    "            obs_, r, done, info = env.step(a)\n",
    "            agent.learn(obs, r, obs_, done)\n",
    "            obs = obs_\n",
    "            score += r\n",
    "            '''if i > 1990:\n",
    "                env.render()'''\n",
    "        score_his.append(score)\n",
    "        avg_score= np.mean(score_his[-100:])\n",
    "        print('episode:', i, 'score: %.2f' %score, 'avg_score: %.2f' %avg_score)\n",
    "        \n",
    "        \n",
    "    filename = 'Lunar-Lander-actor-critic-new-agent-alpha00001-beta00005-2048x512fc-2000games.png'\n",
    "    plotLearning(score_his, filename=filename, window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
